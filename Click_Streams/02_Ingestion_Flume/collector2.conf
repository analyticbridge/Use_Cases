# Define an Avro source:
collector2.sources=r1
collector2.sources.r1.type=avro
collector2.sources.r1.bind=0.0.0.0
collector2.sources.r1.port=4142
collector2.sources.r1.channels=ch1
# Define a file channel using multiple disks for reliability:
collector2.channels=ch1
collector2.channels.ch1.type=FILE
collector2.channels.ch1.checkpointDir=/opt/flume/ch1/checkpoint1
collector2.channels.ch1.dataDirs=/opt/flume/ch1/data1,/opt/flume/ch1/data2
# Define HDFS sinks to persist events as text:
collector2.sinks=k1 k2
collector2.sinks.k1.type=hdfs
collector2.sinks.k1.channel=ch1
# Partition files by date:
#collector2.sinks.k1.hdfs.path=/etl/BI/casualcyclist/clicks/rawlogs/year=%Y/month=%m/day=%d
collector2.sinks.k1.hdfs.path=/etl/BI/casualcyclist/clicks/rawlogs/year=2014/month=10/day=10
collector2.sinks.k1.hdfs.filePrefix=combined
collector2.sinks.k1.hdfs.fileSuffix=.log
collector2.sinks.k1.hdfs.fileType=DataStream
collector2.sinks.k1.hdfs.writeFormat=Text
collector2.sinks.k1.hdfs.batchSize=1000 
# Roll HDFS files every 10000 events or 30 seconds:
collector2.sinks.k1.hdfs.rollInterval=30
collector2.sinks.k1.hdfs.rollCount=10000
collector2.sinks.k1.hdfs.rollSize=0
collector2.sinks.k2.type=hdfs
collector2.sinks.k2.channel=ch1
# Partition files by date:
#collector2.sinks.k2.hdfs.path=/etl/BI/casualcyclist/clicks/rawlogs/year=%Y/month=%m/day=%d
collector2.sinks.k2.hdfs.path=/etl/BI/casualcyclist/clicks/rawlogs/year=2014/month=10/day=10
collector2.sinks.k2.hdfs.filePrefix=combined
collector2.sinks.k2.hdfs.fileSuffix=.log
collector2.sinks.k2.hdfs.fileType=DataStream
collector2.sinks.k2.hdfs.writeFormat=Text
collector2.sinks.k2.hdfs.batchSize=1000 
# Roll HDFS files every 10000 events or 30 seconds:
collector2.sinks.k1.hdfs.rollInterval=30
collector2.sinks.k1.hdfs.rollCount=10000
collector2.sinks.k2.hdfs.rollSize=0
collector2.sinkgroups=g1
collector2.sinkgroups.g1.sinks=k1 k2
collector2.sinkgroups.g1.processor.type=load_balance
collector2.sinkgroups.g1.processor.selector=round_robin
collector2.sinkgroups.g1.processor.backoff=true